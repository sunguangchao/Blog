作业分片

作业分片条件
============

当作业满足分片时，不会立即进行作业分片分配，而是设置需要重新进行分片的标记，等到作业分片获取时，判断有该标记后执行作业分配

```java
//ShardingService.java
/**
 * 设置需要重新分片的标记.
 */
 public void setReshardingFlag() {
    jobNodeStorage.createJobNodeIfNeeded(ShardingNode.NECESSARY);
 }
//JobNodeStorage.java
/**
 * 如果job节点存在，则创建作业节点.
 * <p>
 * <p>如果作业根节点不存在表示作业已经停止, 不再继续创建节点.</p>
 *
 * @param node 作业节点名称
 */
public void createJobNodeIfNeeded(final String node) {
    if (isJobRootNodeExisted() && !isJobNodeExisted(node)) {
        regCenter.persist(jobNodePath.getFullPath(node), "");
    }
}
```

调用`#setReshardingFlag()`方法设置需要重新分片的标记`/${JOB_NAME}/leader/sharding/necessary`。该Zookeeper数据节点是永久节点，存储空串。

设置标记之后通过调用#isNeedSharding()方法即可判断是否需要重新分片
```java
// ShardingService.java
/**
* 判断是否需要重分片.
* 
* @return 是否需要重分片
*/
public boolean isNeedSharding() {
   return jobNodeStorage.isJobNodeExisted(ShardingNode.NECESSARY);
}

// JobNodeStorage.java
/**
* 判断作业节点是否存在.
* 
* @param node 作业节点名称
* @return 作业节点是否存在
*/
public boolean isJobNodeExisted(final String node) {
   return regCenter.isExisted(jobNodePath.getFullPath(node));
}
```
设置需要重新分片有4种情况

第一种，注册作业启动信息时
```java
// SchedulerFacade.java
public void registerStartUpInfo(final boolean enabled) {
   // ... 省略无关代码
   // 设置 需要重新分片的标记
   shardingService.setReshardingFlag();
  // ... 省略无关代码
}
```
第二种，作业总分片数`JobCoreConfiguration.shardingTotalCount`变化时
```java
class ShardingTotalCountChangedJobListener extends AbstractJobListener {
    /**
     * //如果配置平台上面发生了配置改变,并且分片总数发生了改变，则更新本地缓存(setCurrentShardingTotalCount)，同时添加 leader/sharding/necessary节点。
     * --job配置的分片总节点数发生变化监听器（ElasticJob允许通过Web界面修改每个任务配置的分片总数量）。
     * job的配置信息存储在${namespace}/jobname/config节点上，存储内容为json格式的配置信息。
     * 如果${namespace}/jobname/config节点的内容发生变化，zk会触发该节点的节点数据变化事件，
     * 如果zk中存储的分片节点数量与内存中的分片数量(JobRegistry.getInstance())不相同的话，
     * 调用ShardingService设置需要重新分片标记（创建${namespace}/jobname/leader/sharding/necessary持久节点）并更新内存中的分片节点总数。
     *
     * @param path
     * @param eventType
     * @param data
     */
    @Override
    protected void dataChanged(final String path, final Type eventType, final String data) {
        if (configNode.isConfigPath(path) && 0 != JobRegistry.getInstance().getCurrentShardingTotalCount(jobName)) {
            int newShardingTotalCount = LiteJobConfigurationGsonFactory.fromJson(data).getTypeConfig().getCoreConfig().getShardingTotalCount();
            if (newShardingTotalCount != JobRegistry.getInstance().getCurrentShardingTotalCount(jobName)) {
                shardingService.setReshardingFlag();
                JobRegistry.getInstance().setCurrentShardingTotalCount(jobName, newShardingTotalCount);
            }
        }
    }
}
```
第三种，服务器变化时
```java
/**
 * --分片节点（实例数）发生变化事件监听器，当新的分片节点加入或原的分片实例宕机后，需要进行重新分片。
 * 当${namespace}/jobname/servers或${namespace}/jobname/instances路径下的节点数量是否发生变化，如果检测到发生变化，设置需要重新分片标识。
 */
class ListenServersChangedJobListener extends AbstractJobListene
    @Override
    protected void dataChanged(final String path, final Type eventType, final String da{//如果服务没有停止，并且是实例或者服务器发生了改变，需要设置重新分片标记
        if (!JobRegistry.getInstance().isShutdown(jobName) && (isInstanceChange(eventType, path) || isServerChange(path))) {
            shardingService.setReshardingFlag();
        }
  
    private boolean isInstanceChange(final Type eventType, final String path) {
        return instanceNode.isInstancePath(path) && Type.NODE_UPDATED != eventType;
  
    private boolean isServerChange(final String path) {
        return serverNode.isServerPath(path);
    }
}
```
服务器变化有两种情况：
1. `#isServerChange(...)`服务器被开启或禁用
2. `#isInstanceChange(...)`作业节点被新增或移除

第四种，后续介绍

分配作业分片项

调用`ShardingService#shardingIfNecessary()`方法，如果需要分片且当前节点为主节点，则作业分片。

```java
// ShardingService.java
/**
* 如果需要分片且当前节点为主节点, 则作业分片.
* 
* 如果当前无可用节点则不分片.
*/
public void shardingIfNecessary() {
   //获取当前可用实例
   List<JobInstance> availableJobInstances = instanceService.getAvailableJobInstances();
   if (!isNeedSharding() // 判断是否需要重新分片
           || availableJobInstances.isEmpty()) {
       return;
   }
   // 【非主节点】等待 作业分片项分配完成
   if (!leaderService.isLeaderUntilBlock()) { // 判断是否为【主节点】
       blockUntilShardingCompleted();
       return;
   }
   // 【主节点】作业分片项分配
   // 等待 作业未在运行中状态
   waitingOtherJobCompleted();
   //从注册中心读取配置
   LiteJobConfiguration liteJobConfig = configService.load(false);
   int shardingTotalCount = liteJobConfig.getTypeConfig().getCoreConfig().getShardingTotalCount();
   // 设置 作业正在重分片的标记
   log.debug("Job '{}' sharding begin.", jobName);
   //创建临时节点${namespace}/jobname/leader/sharding/processing节点，表示分片正在执行。
   //主节点在分片时持有的节点，如果有此节点，所有的作业执行都将阻塞，直至分片结束，主节点分片结束或主节点崩溃会删除此临时节点
   jobNodeStorage.fillEphemeralJobNode(ShardingNode.PROCESSING, "");
   // 重置 作业分片项信息
   resetShardingInfo(shardingTotalCount);
   // 【事务中】设置 作业分片项信息
   JobShardingStrategy jobShardingStrategy = JobShardingStrategyFactory.getStrategy(liteJobConfig.getJobShardingStrategyClass());
   jobNodeStorage.executeInTransaction(new PersistShardingInfoTransactionExecutionCallback(jobShardingStrategy.sharding(availableJobInstances, jobName, shardingTotalCount)));
   log.debug("Job '{}' sharding complete.", jobName);
}
```

* `jobNodeStorage.fillEphemeralJobNode(ShardingNode.PROCESSING, "")设置作业正在重新分片的标记

`${namespace}/jobname/leader/sharding/processing`。Zookeeper数据节点是临时节点，value存储的是空字符串

`#resetShardingInfo(...)`方法重置作业分片信息

```java
private void resetShardingInfo(final int shardingTotalCount) {
  // 重置 有效的作业分片项
  for (int i = 0; i < shardingTotalCount; i++) {
      jobNodeStorage.removeJobNodeIfExisted(ShardingNode.getInstanceNode(i)); // 移除 `/${JOB_NAME}/sharding/${ITEM_ID}/instance`
      jobNodeStorage.createJobNodeIfNeeded(ShardingNode.ROOT + "/" + i); // 创建 `/${JOB_NAME}/sharding/${ITEM_ID}`
  }
  // 移除 多余的作业分片项
  int actualShardingTotalCount = jobNodeStorage.getJobNodeChildrenKeys(ShardingNode.ROOT).size();
  if (actualShardingTotalCount > shardingTotalCount) {
      for (int i = shardingTotalCount; i < actualShardingTotalCount; i++) {
          jobNodeStorage.removeJobNodeIfExisted(ShardingNode.ROOT + "/" + i); // 移除 `/${JOB_NAME}/sharding/${ITEM_ID}`
      }
  }
}
```

下面的方法就是实现在事务中设置每个节点分配的作业分片项


```java
// PersistShardingInfoTransactionExecutionCallback.java
class PersistShardingInfoTransactionExecutionCallback implements TransactionExecutionCallback {
   
   /**
    * 作业分片项分配结果
    * key：作业节点
    * value：作业分片项
    */
   private final Map<JobInstance, List<Integer>> shardingResults;
   
   @Override
   public void execute(final CuratorTransactionFinal curatorTransactionFinal) throws Exception {
       // 设置 每个节点分配的作业分片项
       for (Map.Entry<JobInstance, List<Integer>> entry : shardingResults.entrySet()) {
           for (int shardingItem : entry.getValue()) {
               curatorTransactionFinal.create().forPath(jobNodePath.getFullPath(ShardingNode.getInstanceNode(shardingItem))
                       , entry.getKey().getJobInstanceId().getBytes()).and();
           }
       }
       // 移除 作业需要重分片的标记、作业正在重分片的标记
       curatorTransactionFinal.delete().forPath(jobNodePath.getFullPath(ShardingNode.NECESSARY)).and();
       curatorTransactionFinal.delete().forPath(jobNodePath.getFullPath(ShardingNode.PROCESSING)).and();
   }
}

// JobNodeStorage.java
/**
* 在事务中执行操作.
* 
* @param callback 执行操作的回调
*/
public void executeInTransaction(final TransactionExecutionCallback callback) {
   try {
       CuratorTransactionFinal curatorTransactionFinal = getClient().inTransaction().check().forPath("/").and();
       callback.execute(curatorTransactionFinal);
       curatorTransactionFinal.commit();
   } catch (final Exception ex) {
       RegExceptionHandler.handleException(ex);
   }
}
```
设置临时数据节点`${namespace}/${jobname}/sharding/${ITEM_ID}/instance`，内容为jobInstanceId。



获取作业分片上下文集合
============

作业执行器AbstractElasticJobExecutor执行作业时，会获取当前服务器的分片上下文进行执行。
```java
@Override
public ShardingContexts getShardingContexts() {
    boolean isFailover = configService.load(true).isFailover();
    if (isFailover) {//--获取分片上下文时，如果启用了故障失效转移机制，优先获取故障失效转移的分片上下文
        /*
        --获取本节点获取的失效分片信息。其基本逻辑是遍历${namespace}/jobname/sharding下的子节点，
        获取该任务当前的所有分片信息，遍历每个节点，获取序号，然后依次判断是否存在(${namespace}/jobname/sharding/{item}/failover),
        并且该节点的内容为当前的实例ID,则加入到分片结果中。
        * */
        List<Integer> failoverShardingItems = failoverService.getLocalFailoverItems();
        if (!failoverShardingItems.isEmpty()) {
            /**
             *-- 根据失效分片序号构建分片上下文环境，执行该分片上的任务，根据分片上下文环境，执行任务。
             * 【AbstractElasticJob#execute(shardingContexts, JobExecutionEvent.ExecutionSource.NORMAL_TRIGGER);】
             * 执行完本次任务调度后，将删除分片的故障标记，待下一次任务调度时重新分片。
             */
            ShardingContexts shardingContexts =  executionContextService.getJobShardingContext(failoverShardingItems);
            shardingContexts.setFailoverContexts(true);
            return shardingContexts;
        }
    }
    shardingService.shardingIfNecessary();//--如果有必要，则执行分片，如果不存在分片信息（第一次分片）或需要重新分片，则执行分片算法。
    //--获取本地的分片信息。遍历所有分片信息${namespace}/jobnsharding/{分片item}下所有instance节点，判断其值jobinstanceId是否与当前的jobInstanceId相等，相等则认为是本节点的分片信息。
    List<Integer> shardingItems = shardingService.getLocalShardingItems();
    if (isFailover) {
        shardingItems.removeAll(failoverService.getLocalTakeOffItems());
    }
    shardingItems.removeAll(executionService.getDisabledItems(shardingItems));//--移除本地禁用分片，本地禁用分片的存储目录为${namespace}/jobnsharding/{分片item}/disable。
    return executionContextService.getJobShardingContext(shardingItems);//--返回当前节点的分片上下文环境,这个主要是根据配置信息（分片参数）与当前的分片实例，构建ShardingContexts对象。
}
```

调用ShardingService#getLocalShardingItems()方法，获取分配在本机的作业分片项，即`/${JOB_NAME}sharding/${ITEM_ID}/instance`

```java
// ShardingService.java
/**
* 获取运行在本作业实例的分片项集合.
* 
* @return 运行在本作业实例的分片项集合
*/
public List<Integer> getLocalShardingItems() {
   if (JobRegistry.getInstance().isShutdown(jobName) || !serverService.isAvailableServer(JobRegistry.getInstance().getJobInstance(jobName).getIp())) {
       return Collections.emptyList();
   }
   return getShardingItems(JobRegistry.getInstance().getJobInstance(jobName).getJobInstanceId());
}

/**
* 获取作业运行实例的分片项集合.
*
* @param jobInstanceId 作业运行实例主键
* @return 作业运行实例的分片项集合
*/
public List<Integer> getShardingItems(final String jobInstanceId) {
   JobInstance jobInstance = new JobInstance(jobInstanceId);
   if (!serverService.isAvailableServer(jobInstance.getIp())) {
       return Collections.emptyList();
   }
   List<Integer> result = new LinkedList<>();
   int shardingTotalCount = configService.load(true).getTypeConfig().getCoreConfig().getShardingTotalCount();
   for (int i = 0; i < shardingTotalCount; i++) {
       // `/${JOB_NAME}/sharding/${ITEM_ID}/instance`
       if (jobInstance.getJobInstanceId().equals(jobNodeStorage.getJobNodeData(ShardingNode.getInstanceNode(i)))) {
           result.add(i);
       }
   }
   return result;
}
```
调用ExecutionContextService#getJobShardingContext(...)方法，获取当前作业服务器分片上下文

```java
// ExecutionContextService.java
public ShardingContexts getJobShardingContext(final List<Integer> shardingItems) {
   LiteJobConfiguration liteJobConfig = configService.load(false);
   // 移除 正在运行中的作业分片项
   removeRunningIfMonitorExecution(liteJobConfig.isMonitorExecution(), shardingItems);
   //
   if (shardingItems.isEmpty()) {
       return new ShardingContexts(buildTaskId(liteJobConfig, shardingItems), liteJobConfig.getJobName(), liteJobConfig.getTypeConfig().getCoreConfig().getShardingTotalCount(), 
               liteJobConfig.getTypeConfig().getCoreConfig().getJobParameter(), Collections.<Integer, String>emptyMap());
   }
   // 解析分片参数
   Map<Integer, String> shardingItemParameterMap = new ShardingItemParameters(liteJobConfig.getTypeConfig().getCoreConfig().getShardingItemParameters()).getMap();
   // 创建 分片上下文集合
   return new ShardingContexts(buildTaskId(liteJobConfig, shardingItems), //
           liteJobConfig.getJobName(), liteJobConfig.getTypeConfig().getCoreConfig().getShardingTotalCount(),
           liteJobConfig.getTypeConfig().getCoreConfig().getJobParameter(),
           getAssignedShardingItemParameterMap(shardingItems, shardingItemParameterMap)); // 获得当前作业节点的分片参数
}
```



http://www.iocoder.cn/Elastic-Job/job-sharding/